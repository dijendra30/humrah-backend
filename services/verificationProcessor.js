// services/verificationProcessor.js - Video Verification Processing Engine (UPDATED)
const faceapi = require('@vladmandic/face-api');
const canvas = require('canvas');
const { Canvas, Image, ImageData } = canvas;
const ffmpeg = require('fluent-ffmpeg');
const sharp = require('sharp');
const fs = require('fs').promises;
const fsSync = require('fs');
const path = require('path');
const axios = require('axios');
const { cloudinary } = require('../config/cloudinary');

// Patch face-api to use node-canvas
faceapi.env.monkeyPatch({ Canvas, Image, ImageData });

// =============================================
// CONFIGURATION - ‚úÖ UPDATED FOR BETTER APPROVAL RATES
// =============================================
const CONFIG = {
  MODELS_PATH: path.join(__dirname, '../models/face-detection'),
  TEMP_DIR: path.join(__dirname, '../temp'),
  
  // ‚úÖ More lenient thresholds
  FACE_MATCH_THRESHOLD: parseFloat(process.env.FACE_MATCH_THRESHOLD) || 0.50,  // Lowered from 0.60
  LIVENESS_THRESHOLD: parseFloat(process.env.LIVENESS_THRESHOLD) || 0.50,      // Lowered from 0.70
  
  MIN_FRAMES: 8,                     // Lowered from 10
  MAX_FRAMES: 20,
  FRAME_INTERVAL_MS: 300
};

// =============================================
// INITIALIZE MODELS (Load once at startup)
// =============================================
let modelsLoaded = false;

async function loadModels() {
  if (modelsLoaded) return;
  
  try {
    console.log('üì¶ Loading face-api models...');
    console.log('üìÅ Models path:', CONFIG.MODELS_PATH);
    
    // Check if models directory exists
    if (!fsSync.existsSync(CONFIG.MODELS_PATH)) {
      console.log('üìÅ Creating models directory...');
      fsSync.mkdirSync(CONFIG.MODELS_PATH, { recursive: true });
    }
    
    // Check for required model files
    const requiredFiles = [
      'ssd_mobilenetv1_model-weights_manifest.json',
      'face_landmark_68_model-weights_manifest.json',
      'face_recognition_model-weights_manifest.json'
    ];
    
    const missingFiles = requiredFiles.filter(file => 
      !fsSync.existsSync(path.join(CONFIG.MODELS_PATH, file))
    );
    
    if (missingFiles.length > 0) {
      console.log('‚ö†Ô∏è Missing model files:', missingFiles);
      console.log('üì• Downloading models automatically...');
      
      try {
        const { downloadModels } = require('../scripts/download-models');
        await downloadModels();
        console.log('‚úÖ Models downloaded successfully!');
      } catch (downloadError) {
        console.error('‚ùå Failed to download models:', downloadError.message);
        console.error('üí° Please run manually: node scripts/download-models.js');
        throw new Error('Models not available and auto-download failed');
      }
    }
    
    // Verify files exist after download
    const stillMissing = requiredFiles.filter(file => 
      !fsSync.existsSync(path.join(CONFIG.MODELS_PATH, file))
    );
    
    if (stillMissing.length > 0) {
      throw new Error(`Models still missing after download: ${stillMissing.join(', ')}`);
    }
    
    // Load models
    console.log('üîÑ Loading ssdMobilenetv1...');
    await faceapi.nets.ssdMobilenetv1.loadFromDisk(CONFIG.MODELS_PATH);
    
    console.log('üîÑ Loading faceLandmark68Net...');
    await faceapi.nets.faceLandmark68Net.loadFromDisk(CONFIG.MODELS_PATH);
    
    console.log('üîÑ Loading faceRecognitionNet...');
    await faceapi.nets.faceRecognitionNet.loadFromDisk(CONFIG.MODELS_PATH);
    
    modelsLoaded = true;
    console.log('‚úÖ Face-api models loaded successfully!');
    
  } catch (error) {
    console.error('‚ùå Error loading models:', error);
    console.error('‚ö†Ô∏è Verification system will not work until models are loaded');
    console.error('üí° Try running: node scripts/download-models.js');
    throw new Error('Failed to load face recognition models');
  }
}

// Load models on module import
loadModels().catch(err => {
  console.error('‚ùå Failed to load models on startup:', err.message);
});

// =============================================
// MAIN PROCESSING FUNCTION
// =============================================
async function processVerificationVideo(cloudinaryPublicId, user, session) {
  const startTime = Date.now();
  let tempVideoPath = null;
  let tempFramesDir = null;
  
  try {
    console.log(`\nüé¨ [Verification] Processing session ${session.sessionId}`);
    
    if (!modelsLoaded) {
      console.log('‚ö†Ô∏è Models not loaded yet, attempting to load...');
      await loadModels();
    }
    
    // STEP 1: Download video
    console.log('üì• Step 1: Downloading video from Cloudinary...');
    tempVideoPath = await downloadVideo(cloudinaryPublicId);
    console.log(`‚úÖ Video downloaded: ${tempVideoPath}`);
    
    // STEP 2: Extract frames
    console.log('üéûÔ∏è Step 2: Extracting frames from video...');
    tempFramesDir = await extractFrames(tempVideoPath);
    const framePaths = await fs.readdir(tempFramesDir);
    console.log(`‚úÖ Extracted ${framePaths.length} frames`);
    
    if (framePaths.length < CONFIG.MIN_FRAMES) {
      throw new Error(`Insufficient frames: ${framePaths.length} < ${CONFIG.MIN_FRAMES}`);
    }
    
    // STEP 3: Liveness Detection
    console.log('üëÅÔ∏è Step 3: Performing liveness detection...');
    const livenessResult = await detectLiveness(tempFramesDir, framePaths);
    console.log(`‚úÖ Liveness score: ${(livenessResult.score * 100).toFixed(1)}%`);
    
    if (!livenessResult.passed) {
      return {
        decision: 'REJECTED',
        confidence: 0,
        livenessScore: livenessResult.score,
        faceMatchScore: null,
        rejectionReason: livenessResult.reason,
        faceEmbedding: null
      };
    }
    
    // STEP 4: Face Detection & Embedding
    console.log('üîç Step 4: Detecting faces and generating embedding...');
    const faceResult = await extractBestFace(tempFramesDir, framePaths);
    
    if (!faceResult.success) {
      return {
        decision: 'REJECTED',
        confidence: 0,
        livenessScore: livenessResult.score,
        faceMatchScore: null,
        rejectionReason: faceResult.reason,
        faceEmbedding: null
      };
    }
    
    console.log(`‚úÖ Face detected, embedding generated`);
    
    // STEP 5: Face Matching
    console.log('üé≠ Step 5: Matching face with profile photo...');
    let faceMatchScore = null;
    
    if (user.profilePhoto) {
      try {
        faceMatchScore = await matchWithProfilePhoto(
          faceResult.embedding,
          user.profilePhoto
        );
        console.log(`‚úÖ Face match score: ${(faceMatchScore * 100).toFixed(1)}%`);
      } catch (error) {
        console.error('‚ö†Ô∏è Face matching failed:', error.message);
      }
    } else {
      console.log('‚ÑπÔ∏è No profile photo to match against');
    }
    
    // STEP 6: Decision Logic
    console.log('‚öñÔ∏è Step 6: Making decision...');
    const decision = makeDecision(livenessResult.score, faceMatchScore);
    
    const processingTime = Date.now() - startTime;
    
    console.log(`\nüìä [Verification] Results for session ${session.sessionId}:`);
    console.log(`   Decision: ${decision.decision}`);
    console.log(`   Confidence: ${(decision.confidence * 100).toFixed(1)}%`);
    console.log(`   Liveness: ${(livenessResult.score * 100).toFixed(1)}%`);
    console.log(`   Face Match: ${faceMatchScore ? (faceMatchScore * 100).toFixed(1) + '%' : 'N/A'}`);
    console.log(`   Processing Time: ${processingTime}ms`);
    
    return {
      decision: decision.decision,
      confidence: decision.confidence,
      livenessScore: livenessResult.score,
      faceMatchScore: faceMatchScore,
      rejectionReason: decision.rejectionReason,
      faceEmbedding: faceResult.embedding
    };
    
  } catch (error) {
    console.error('‚ùå [Verification] Processing error:', error);
    
    return {
      decision: 'FAILED',
      confidence: 0,
      livenessScore: null,
      faceMatchScore: null,
      rejectionReason: `Processing error: ${error.message}`,
      faceEmbedding: null
    };
    
  } finally {
    try {
      if (tempVideoPath) {
        await fs.unlink(tempVideoPath);
        console.log('üóëÔ∏è Temporary video deleted');
      }
      
      if (tempFramesDir) {
        await fs.rm(tempFramesDir, { recursive: true, force: true });
        console.log('üóëÔ∏è Temporary frames deleted');
      }
    } catch (cleanupError) {
      console.error('‚ö†Ô∏è Cleanup error:', cleanupError);
    }
  }
}

// =============================================
// HELPER FUNCTIONS
// =============================================

async function downloadVideo(publicId) {
  try {
    console.log(`üì• [Download] Getting authenticated URL for: ${publicId}`);
    
    const videoUrl = cloudinary.url(publicId, {
      resource_type: 'video',
      type: 'authenticated',
      sign_url: true,
      secure: true
    });
    
    console.log(`üåê [Download] Downloading from Cloudinary...`);
    
    await fs.mkdir(CONFIG.TEMP_DIR, { recursive: true });
    
    const tempPath = path.join(CONFIG.TEMP_DIR, `${Date.now()}_video.mp4`);
    
    const response = await axios({
      method: 'get',
      url: videoUrl,
      responseType: 'stream',
      timeout: 60000
    });
    
    const writer = fsSync.createWriteStream(tempPath);
    response.data.pipe(writer);
    
    return new Promise((resolve, reject) => {
      writer.on('finish', () => {
        console.log(`‚úÖ [Download] Video saved to: ${tempPath}`);
        resolve(tempPath);
      });
      writer.on('error', (err) => {
        console.error(`‚ùå [Download] Write error:`, err);
        reject(err);
      });
    });
  } catch (error) {
    console.error(`‚ùå [Download] Failed to download video:`, error.message);
    throw error;
  }
}

async function extractFrames(videoPath) {
  const framesDir = path.join(CONFIG.TEMP_DIR, `frames_${Date.now()}`);
  await fs.mkdir(framesDir, { recursive: true });
  
  return new Promise((resolve, reject) => {
    console.log(`üéûÔ∏è [Frames] Extracting frames to: ${framesDir}`);
    
    ffmpeg(videoPath)
      .outputOptions([
        `-vf fps=1000/${CONFIG.FRAME_INTERVAL_MS}`,
        `-vframes ${CONFIG.MAX_FRAMES}`
      ])
      .output(path.join(framesDir, 'frame_%03d.jpg'))
      .on('end', () => {
        console.log(`‚úÖ [Frames] Frame extraction complete`);
        resolve(framesDir);
      })
      .on('error', (err) => {
        console.error(`‚ùå [Frames] Extraction failed:`, err.message);
        reject(err);
      })
      .run();
  });
}

/**
 * ‚úÖ UPDATED: More lenient liveness detection
 */
async function detectLiveness(framesDir, framePaths) {
  const frames = [];
  
  console.log(`üëÅÔ∏è [Liveness] Analyzing ${Math.min(framePaths.length, 15)} frames...`);
  
  for (const framePath of framePaths.slice(0, 15)) {
    const fullPath = path.join(framesDir, framePath);
    const img = await canvas.loadImage(fullPath);
    const detection = await faceapi
      .detectSingleFace(img)
      .withFaceLandmarks();
    
    if (detection) {
      frames.push(detection);
    }
  }
  
  if (frames.length < 3) {  // ‚úÖ Lowered from 5
    return {
      passed: false,
      score: 0,
      reason: 'Insufficient face detections for liveness check'
    };
  }
  
  // Check 1: Blink detection
  const eyeAspectRatios = frames.map(f => calculateEyeAspectRatio(f.landmarks));
  const earVariance = calculateVariance(eyeAspectRatios);
  const blinkDetected = earVariance > 0.005;  // ‚úÖ Lowered from 0.01
  
  // Check 2: Head movement
  const yawAngles = frames.map(f => estimateYawAngle(f.landmarks));
  const yawVariance = calculateVariance(yawAngles);
  const headMovement = yawVariance > 0.02;  // ‚úÖ Lowered from 0.05
  
  // Check 3: Pixel variance
  const firstFramePath = path.join(framesDir, framePaths[0]);
  const pixelVariance = await calculatePixelVariance(firstFramePath);
  const notFlatPhoto = pixelVariance > 300;  // ‚úÖ Lowered from 500
  
  console.log(`   üëÅÔ∏è  Blink detected: ${blinkDetected} (variance: ${earVariance.toFixed(4)})`);
  console.log(`   üîÑ Head movement: ${headMovement} (variance: ${yawVariance.toFixed(4)})`);
  console.log(`   üì∑ Not flat photo: ${notFlatPhoto} (variance: ${pixelVariance.toFixed(2)})`);
  
  // ‚úÖ More generous scoring
  let score = 0;
  let checksPassedCount = 0;
  
  if (blinkDetected) { score += 0.35; checksPassedCount++; }
  if (headMovement) { score += 0.35; checksPassedCount++; }
  if (notFlatPhoto) { score += 0.30; checksPassedCount++; }
  
  // ‚úÖ Pass if ANY 2 out of 3 checks pass OR score >= 0.50
  const passed = checksPassedCount >= 2 || score >= CONFIG.LIVENESS_THRESHOLD;
  
  console.log(`   ‚úÖ Liveness checks passed: ${checksPassedCount}/3`);
  console.log(`   üìä Final liveness score: ${(score * 100).toFixed(1)}%`);
  console.log(`   ${passed ? '‚úÖ PASSED' : '‚ùå FAILED'} liveness detection`);
  
  return {
    passed,
    score,
    reason: passed ? null : 'Failed liveness detection (possible photo/video spoof)'
  };
}

async function extractBestFace(framesDir, framePaths) {
  let bestFace = null;
  let bestQuality = 0;
  
  console.log(`üîç [Face] Analyzing frames for best face...`);
  
  for (const framePath of framePaths) {
    const fullPath = path.join(framesDir, framePath);
    const img = await canvas.loadImage(fullPath);
    
    const detections = await faceapi
      .detectAllFaces(img)
      .withFaceLandmarks()
      .withFaceDescriptors();
    
    if (detections.length === 0) continue;
    if (detections.length > 1) continue;
    
    const detection = detections[0];
    const quality = detection.detection.score * detection.detection.box.area;
    
    if (quality > bestQuality) {
      bestQuality = quality;
      bestFace = detection;
    }
  }
  
  if (!bestFace) {
    return {
      success: false,
      reason: 'No clear single face detected in video'
    };
  }
  
  console.log(`‚úÖ [Face] Best face found with quality: ${bestQuality.toFixed(2)}`);
  
  return {
    success: true,
    embedding: Array.from(bestFace.descriptor),
    quality: bestQuality
  };
}

async function matchWithProfilePhoto(verificationEmbedding, profilePhotoUrl) {
  console.log(`üé≠ [Match] Downloading profile photo...`);
  
  const response = await axios({
    method: 'get',
    url: profilePhotoUrl,
    responseType: 'arraybuffer',
    timeout: 30000
  });
  
  const buffer = Buffer.from(response.data);
  const img = await canvas.loadImage(buffer);
  
  console.log(`üîç [Match] Detecting face in profile photo...`);
  
  const detection = await faceapi
    .detectSingleFace(img)
    .withFaceLandmarks()
    .withFaceDescriptor();
  
  if (!detection) {
    throw new Error('No face detected in profile photo');
  }
  
  const profileEmbedding = Array.from(detection.descriptor);
  const similarity = calculateCosineSimilarity(verificationEmbedding, profileEmbedding);
  
  console.log(`‚úÖ [Match] Similarity calculated: ${(similarity * 100).toFixed(2)}%`);
  
  return similarity;
}

/**
 * ‚úÖ UPDATED: More lenient decision logic
 */
function makeDecision(livenessScore, faceMatchScore) {
  console.log(`\n‚öñÔ∏è [Decision] Making final decision...`);
  console.log(`   Liveness Score: ${(livenessScore * 100).toFixed(1)}%`);
  console.log(`   Face Match Score: ${faceMatchScore ? (faceMatchScore * 100).toFixed(1) + '%' : 'N/A'}`);
  
  // CASE 1: No profile photo - only need liveness
  if (faceMatchScore === null) {
    if (livenessScore >= CONFIG.LIVENESS_THRESHOLD) {
      console.log(`   ‚úÖ APPROVED - Liveness passed (${(livenessScore * 100).toFixed(1)}%)`);
      return {
        decision: 'APPROVED',
        confidence: livenessScore,
        rejectionReason: null
      };
    } else {
      console.log(`   ‚ùå REJECTED - Liveness too low (${(livenessScore * 100).toFixed(1)}% < 50%)`);
      return {
        decision: 'REJECTED',
        confidence: livenessScore,
        rejectionReason: 'Liveness check failed - please record a clearer video'
      };
    }
  }
  
  // CASE 2: Has profile photo - need both liveness AND face match
  const combinedScore = (livenessScore * 0.4) + (faceMatchScore * 0.6);
  
  console.log(`   Combined Score: ${(combinedScore * 100).toFixed(1)}%`);
  
  // ‚úÖ APPROVE: Face match >= 65% AND liveness >= 45%
  if (faceMatchScore >= 0.65 && livenessScore >= 0.45) {
    console.log(`   ‚úÖ APPROVED - Both checks passed`);
    return {
      decision: 'APPROVED',
      confidence: combinedScore,
      rejectionReason: null
    };
  }
  
  // ‚úÖ APPROVE: High face match (>= 70%) even with lower liveness
  else if (faceMatchScore >= 0.70 && livenessScore >= 0.35) {
    console.log(`   ‚úÖ APPROVED - Strong face match compensates for liveness`);
    return {
      decision: 'APPROVED',
      confidence: combinedScore,
      rejectionReason: null
    };
  }
  
  // ‚úÖ MANUAL REVIEW: Moderate scores
  else if (faceMatchScore >= 0.50 && faceMatchScore < 0.65) {
    console.log(`   ‚è≥ MANUAL REVIEW - Face match in review range`);
    return {
      decision: 'MANUAL_REVIEW',
      confidence: combinedScore,
      rejectionReason: 'Face match score requires manual review'
    };
  }
  
  // ‚úÖ REJECT: Low face match
  else {
    console.log(`   ‚ùå REJECTED - Face match too low`);
    return {
      decision: 'REJECTED',
      confidence: combinedScore,
      rejectionReason: 'Face does not match profile photo - please record a clearer video'
    };
  }
}

// =============================================
// UTILITY FUNCTIONS
// =============================================

function calculateEyeAspectRatio(landmarks) {
  const leftEye = landmarks.getLeftEye();
  const rightEye = landmarks.getRightEye();
  
  const leftEAR = eyeAspectRatio(leftEye);
  const rightEAR = eyeAspectRatio(rightEye);
  
  return (leftEAR + rightEAR) / 2;
}

function eyeAspectRatio(eye) {
  const p1 = eye[1];
  const p2 = eye[5];
  const p3 = eye[2];
  const p4 = eye[4];
  const p5 = eye[0];
  const p6 = eye[3];
  
  const vertical1 = distance(p1, p5);
  const vertical2 = distance(p2, p6);
  const horizontal = distance(p3, p4);
  
  return (vertical1 + vertical2) / (2.0 * horizontal);
}

function distance(p1, p2) {
  return Math.sqrt(Math.pow(p2.x - p1.x, 2) + Math.pow(p2.y - p1.y, 2));
}

function estimateYawAngle(landmarks) {
  const nose = landmarks.getNose();
  const leftEye = landmarks.getLeftEye();
  const rightEye = landmarks.getRightEye();
  
  const noseTip = nose[3];
  const leftEyeCenter = centroid(leftEye);
  const rightEyeCenter = centroid(rightEye);
  
  const eyeDistance = distance(leftEyeCenter, rightEyeCenter);
  const noseOffset = noseTip.x - (leftEyeCenter.x + rightEyeCenter.x) / 2;
  
  return noseOffset / eyeDistance;
}

function centroid(points) {
  const sum = points.reduce((acc, p) => ({ x: acc.x + p.x, y: acc.y + p.y }), { x: 0, y: 0 });
  return { x: sum.x / points.length, y: sum.y / points.length };
}

function calculateVariance(values) {
  const mean = values.reduce((a, b) => a + b, 0) / values.length;
  const variance = values.reduce((acc, val) => acc + Math.pow(val - mean, 2), 0) / values.length;
  return variance;
}

async function calculatePixelVariance(imagePath) {
  const { data, info } = await sharp(imagePath)
    .greyscale()
    .raw()
    .toBuffer({ resolveWithObject: true });
  
  const pixels = Array.from(data);
  return calculateVariance(pixels);
}

function calculateCosineSimilarity(vec1, vec2) {
  const dotProduct = vec1.reduce((sum, val, i) => sum + val * vec2[i], 0);
  const mag1 = Math.sqrt(vec1.reduce((sum, val) => sum + val * val, 0));
  const mag2 = Math.sqrt(vec2.reduce((sum, val) => sum + val * val, 0));
  return dotProduct / (mag1 * mag2);
}

// =============================================
// EXPORTS
// =============================================
module.exports = {
  processVerificationVideo,
  loadModels
};
